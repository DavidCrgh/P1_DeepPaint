{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.0 64-bit"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "dfa02dee088c29a9ad5235ddd12ddae865f14210b2ad5609320376f4ee592e8c"
    },
    "colab": {
      "name": "DeepPaint.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "877a24dca36048a280704d4f6ed60532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_550384e8f37c437ea9c45540d1c5e682",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f0e9c50be05640f6994ec657303b4130",
              "IPY_MODEL_27eba417aa4a4d9babea849f31c5cab4",
              "IPY_MODEL_d44f2dbc227f44108dd443bb830e632b"
            ]
          }
        },
        "550384e8f37c437ea9c45540d1c5e682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f0e9c50be05640f6994ec657303b4130": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1c9d5977de8d49828902e68ad8306b99",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d79965997bfa4a0faaeda55d3adfe901"
          }
        },
        "27eba417aa4a4d9babea849f31c5cab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_3a275f3658b7492baf50fa7f053b8455",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1999639040,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1999639040,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c13539cddcf44efa95f6ea116fa636e2"
          }
        },
        "d44f2dbc227f44108dd443bb830e632b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a45714e33ddd48018db181fe5e052907",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1999639552/? [03:37&lt;00:00, 9194437.34it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c6a547aee59f409989a49161d0315d22"
          }
        },
        "1c9d5977de8d49828902e68ad8306b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d79965997bfa4a0faaeda55d3adfe901": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3a275f3658b7492baf50fa7f053b8455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c13539cddcf44efa95f6ea116fa636e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a45714e33ddd48018db181fe5e052907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c6a547aee59f409989a49161d0315d22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRltVoJiHVa3"
      },
      "source": [
        "!pip install fastai --upgrade\n",
        "!pip install pytorch-ignite --upgrade\n",
        "!pip install scipy --upgrade\n",
        "!pip install scikit-image --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmCttJYcIJsD",
        "outputId": "8658bd4b-c7b9-42a3-e0ae-c7ac65969282"
      },
      "source": [
        "# Run to mount Google Drive to Colab instance\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Flee8G1pNTQZ",
        "outputId": "12c55948-948d-4a86-f892-4724aa07ced5"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 10 18:29:42 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.63.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   67C    P0    29W /  70W |  14576MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "wP7z9Py_HRc5"
      },
      "source": [
        "# Library imports\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torchvision.models.resnet import resnet18\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from fastai.vision.learner import create_body\n",
        "from fastai.vision.models.unet import DynamicUnet\n",
        "\n",
        "from ignite.engine import Events, Engine, create_supervised_trainer, create_supervised_evaluator\n",
        "from ignite.metrics import Accuracy, Loss, PSNR, SSIM, FID\n",
        "\n",
        "from skimage.color import rgb2lab, lab2rgb\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "xqThaZdpHRc8"
      },
      "source": [
        "# Hyperparameters\n",
        "NET_IMG_SIZE = 256\n",
        "LEARNING_RATE = 1e-3\n",
        "BATCH_SIZE = 25\n",
        "\n",
        "# Settings\n",
        "CHECKPOINT_PATH = 'drive/MyDrive/DeepPaint/Checkpoints'\n",
        "METRICS_LOG_PATH = 'drive/MyDrive/DeepPaint/Metrics'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1_boZOKHRc8"
      },
      "source": [
        "# Auxiliary functions\n",
        "def rgbfromlab(L, ab):\n",
        "    L = (L + 1.) * 50.\n",
        "    ab = ab * 110.\n",
        "    \n",
        "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
        "    rgb_imgs = []\n",
        "    for img in Lab:\n",
        "        img_rgb = lab2rgb(img)\n",
        "        rgb_imgs.append(img_rgb)\n",
        "\n",
        "    \n",
        "    return torch.from_numpy(np.stack(rgb_imgs, axis=0)).permute(0,3,1,2)\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, model_name, path):\n",
        "    print('Creating checkpoint...')\n",
        "    \n",
        "    timestamp_str = get_current_timestamp()\n",
        "    filename = f'{model_name}_E{epoch}_{timestamp_str}.pth'\n",
        "\n",
        "    fullpath = os.path.join(path, filename)\n",
        "\n",
        "    torch.save({\n",
        "       'epoch': epoch,\n",
        "       'model_state_dict': model.state_dict(),\n",
        "       'optimizer_state_dict': optimizer.state_dict()\n",
        "    }, fullpath)\n",
        "\n",
        "    print(f'Done! Created checkpoint file: {fullpath}\\n')\n",
        "\n",
        "def log_metrics_to_csv(epoch, iter, metrics, header, filename, path, ext = '.csv', verbose = False):\n",
        "    if verbose: print('Saving metrics...')\n",
        "\n",
        "    fullpath = os.path.join(path, filename + ext)\n",
        "\n",
        "    line = np.insert(metrics, 0, [epoch, iter])\n",
        "\n",
        "    if not os.path.exists(fullpath):\n",
        "        f = open(fullpath, mode='w')\n",
        "        f.write(f'{header}\\n')\n",
        "        f.close()\n",
        "\n",
        "    with open(fullpath, 'ab') as f:\n",
        "        np.savetxt(f, [line], fmt='%7.5f', delimiter=',')\n",
        "\n",
        "    if verbose: print(f'Done! Saved to {fullpath}')\n",
        "\n",
        "# TODO: find out how to always print timestamp for UTC-6\n",
        "def get_current_timestamp():\n",
        "    return datetime.datetime.now().strftime(\"%d-%m-%Y_%H-%M-%S\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtP60YbJHRc9"
      },
      "source": [
        "class VOCColorization(datasets.VOCDetection):\n",
        "    def __init__(\n",
        "        self, \n",
        "        root = 'data', \n",
        "        year = '2012', \n",
        "        image_set = 'train', \n",
        "        download = True, \n",
        "        transform = None, \n",
        "        target_transform = None, \n",
        "        transforms = None):\n",
        "\n",
        "        super().__init__(root, year=year, image_set=image_set, download=download, transform=transform, target_transform=target_transform, transforms=transforms)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # For now we can discard the annotation/label, we can modify this method later should we need it\n",
        "        # Note that the variable length of the annotations causes problems with the dataloader when retrieving\n",
        "        # a batch\n",
        "        target, label = super().__getitem__(index)\n",
        "\n",
        "        lab_image = self.preprocess_image(target) # target is the original PIL RGB Image\n",
        "\n",
        "        return lab_image, transforms.ToTensor()(target) # Convert PIL Image to Tensor to use a dataloader\n",
        "\n",
        "    \"\"\"\n",
        "    Takes a PIL Image in RGB mode and transfers it to CIELab color space.\n",
        "\n",
        "    \"\"\"\n",
        "    def preprocess_image(self, img):\n",
        "        #resize_transform = transforms.Resize((NET_IMG_SIZE, NET_IMG_SIZE))\n",
        "\n",
        "        #img = resize_transform(img)\n",
        "        img = np.array(img)\n",
        "        img_lab = rgb2lab(img).astype(\"float32\") # Convert RGB to Lab color space\n",
        "        img_lab = transforms.ToTensor()(img_lab)\n",
        "\n",
        "        # Adjust all channels to range [-1,1]\n",
        "        img_lab[[0], ...] = img_lab[[0], ...] / 50. - 1. # L\n",
        "        img_lab[[1,2], ...] = img_lab[[1,2], ...] / 110. # ab\n",
        "\n",
        "        return img_lab\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152,
          "referenced_widgets": [
            "877a24dca36048a280704d4f6ed60532",
            "550384e8f37c437ea9c45540d1c5e682",
            "f0e9c50be05640f6994ec657303b4130",
            "27eba417aa4a4d9babea849f31c5cab4",
            "d44f2dbc227f44108dd443bb830e632b",
            "1c9d5977de8d49828902e68ad8306b99",
            "d79965997bfa4a0faaeda55d3adfe901",
            "3a275f3658b7492baf50fa7f053b8455",
            "c13539cddcf44efa95f6ea116fa636e2",
            "a45714e33ddd48018db181fe5e052907",
            "c6a547aee59f409989a49161d0315d22"
          ]
        },
        "id": "AB7fia3nHRc9",
        "outputId": "c0f511b0-86bd-46e0-faf3-fd126068b178"
      },
      "source": [
        "# Download the Pascal VOC2012 datasets\n",
        "# For now, we'll use the 'train' image subset as training data and 'val' as the testing set.\n",
        "training_data = VOCColorization(\n",
        "    'data', \n",
        "    year='2012', \n",
        "    image_set='train',\n",
        "    transform=transforms.Resize((NET_IMG_SIZE, NET_IMG_SIZE)), \n",
        "    download=True)\n",
        "\n",
        "test_data = VOCColorization(\n",
        "    'data', \n",
        "    year='2012', \n",
        "    image_set='val',\n",
        "    transform=transforms.Resize((NET_IMG_SIZE, NET_IMG_SIZE)),\n",
        "    download=True)\n",
        "\n",
        "print(f'Training dataset size = {len(training_data)}')\n",
        "print(f'Testing dataset size = {len(test_data)}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar to data/VOCtrainval_11-May-2012.tar\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "877a24dca36048a280704d4f6ed60532",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/1999639040 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/VOCtrainval_11-May-2012.tar to data\n",
            "Using downloaded and verified file: data/VOCtrainval_11-May-2012.tar\n",
            "Extracting data/VOCtrainval_11-May-2012.tar to data\n",
            "Training dataset size = 5717\n",
            "Testing dataset size = 5823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lspY3ORGHRc-"
      },
      "source": [
        "# Sample code to visualize random Colorization dataset images\n",
        "lab_img, rgb_img = training_data[random.randint(0, len(training_data))]\n",
        "print(f'lab_img = {lab_img.shape}\\nrgb_img={rgb_img.shape}')\n",
        "\n",
        "# Slice off L and ab channels\n",
        "L = lab_img[[0], ...]\n",
        "ab = lab_img[[1,2], ...]\n",
        "\n",
        "# Convert from 1xHxW array to HxW so we can display it with PyPlot\n",
        "new_L = L[0, :, :]\n",
        "print(new_L)\n",
        "\n",
        "# Display our images using pyplot\n",
        "rows, cols = 1, 2\n",
        "fig = plt.figure(figsize=(12,12))\n",
        "\n",
        "fig.add_subplot(rows, cols, 1)\n",
        "plt.title(f'Original (Resized to {NET_IMG_SIZE}x{NET_IMG_SIZE})')\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(transforms.ToPILImage()(rgb_img))\n",
        "\n",
        "fig.add_subplot(rows, cols, 2)\n",
        "plt.title('L* Channel')\n",
        "plt.axis(\"off\")\n",
        "plt.imshow(new_L, cmap='gray')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7N28jm1HRc_"
      },
      "source": [
        "# Create dataloaders for our datasets\n",
        "training_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDoF0urAHRc_"
      },
      "source": [
        "#lab_img, rgb_img = next(iter(training_dataloader))\n",
        "\n",
        "#print(f'lab_img.shape = {lab_img.shape}\\nrgb_img.shape = {rgb_img.shape}')\n",
        "#print(f'lab_img = {lab_img}\\n')\n",
        "#print(f'rgb_img = {rgb_img}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUPaNhqfHRdA"
      },
      "source": [
        "body = create_body(resnet18, pretrained=True, n_in=1, cut=-2)\n",
        "model = DynamicUnet(body, n_out=2, img_size=(NET_IMG_SIZE, NET_IMG_SIZE))\n",
        "model_name = 'dyunet' # Used for checkpointing\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "#print(model)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rBpVc3OHRdA"
      },
      "source": [
        "# Test to ensure that our model accepts inputs and returns outputs of the correct shape\n",
        "lab_img, rgb_img = next(iter(training_dataloader))\n",
        "\n",
        "L_channel = lab_img[:,[0], ...]\n",
        "ab_channels = lab_img[:,[1,2], ...]\n",
        "\n",
        "print(f'L_channel.shape = {L_channel.shape}\\n')\n",
        "print(f'ab_channel.shape = {ab_channels.shape}\\n')\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    L_channel = L_channel.to(device)\n",
        "\n",
        "    ab_hat = model(L_channel)\n",
        "\n",
        "    print(f'ab_hat = {ab_hat.shape}')\n",
        "\n",
        "    colorized_img = rgbfromlab(L_channel, ab_hat)\n",
        "\n",
        "    #pil_img = transforms.ToPILImage()(colorized_img[0])\n",
        "\n",
        "    plt.imshow(colorized_img[0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFm-Y8y84Cjd"
      },
      "source": [
        "# Create some smaller datasets and loaders to test the following code\n",
        "smaller_test_ds = [training_data[i] for i in range(100)]\n",
        "smaller_test_dl = DataLoader(smaller_test_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "smaller_val_ds = [test_data[i] for i in range(100)]\n",
        "smaller_val_dl = DataLoader(smaller_val_ds, batch_size=BATCH_SIZE, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGxOpZBGHRdB"
      },
      "source": [
        "# Train a Unet (ResNet18 backbone) with L2 Loss\n",
        "# Define optimizer and loss function\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "loss_func = nn.MSELoss()\n",
        "\n",
        "# Define training loop step and training Engine\n",
        "def train_step(engine, batch):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    lab_img, rgb_img = batch[0].to(device), batch[1]\n",
        "\n",
        "    L_channel = lab_img[:,[0], ...]\n",
        "    ab_channels = lab_img[:,[1,2], ...]\n",
        "\n",
        "    ab_prediction = model(L_channel)\n",
        "\n",
        "    loss = loss_func(ab_prediction, ab_channels)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    \n",
        "    return loss.item()\n",
        "\n",
        "trainer = Engine(train_step)\n",
        "\n",
        "# Define validation loop step and validation Engine\n",
        "def validation_step(engine, batch):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        lab_img, rgb_img = batch[0].to(device), batch[1]\n",
        "\n",
        "        L_channel = lab_img[:,[0], ...]\n",
        "        ab_channels = lab_img[:,[1,2], ...]\n",
        "\n",
        "        ab_prediction = model(L_channel)\n",
        "\n",
        "        # Disable skimage warnings when converting lab -> rgb due to out-of-range\n",
        "        # values (this is expected)\n",
        "        with warnings.catch_warnings():\n",
        "          warnings.simplefilter('ignore')\n",
        "          rgb_pred = rgbfromlab(L_channel, ab_prediction)\n",
        "\n",
        "        return rgb_pred, rgb_img\n",
        "\n",
        "evaluator = Engine(validation_step)\n",
        "\n",
        "# Define and attach metrics to engines\n",
        "l2_loss = Loss(loss_func)\n",
        "l2_loss.attach(evaluator, 'l2_loss')\n",
        "\n",
        "psnr = PSNR(data_range=255.0)\n",
        "psnr.attach(evaluator, 'psnr')\n",
        "\n",
        "ssim = SSIM(data_range=255.0)\n",
        "ssim = ssim.attach(evaluator, 'ssim')\n",
        "\n",
        "fid = FID()\n",
        "fid.attach(evaluator, 'fid')\n",
        "\n",
        "# Add event handlers to trainer engine\n",
        "@trainer.on(Events.ITERATION_COMPLETED)\n",
        "def log_training_loss(engine):\n",
        "    epoch = engine.state.epoch\n",
        "    iter = engine.state.iteration\n",
        "    loss = engine.state.output\n",
        "\n",
        "    metrics = np.array([loss])\n",
        "\n",
        "    print(f\"Epoch[{epoch}] Iter[{iter}] Loss: {loss:.5f}\")\n",
        "    log_metrics_to_csv(epoch, iter, metrics, \n",
        "                       header='epoch,iter,loss', \n",
        "                       filename='training_loss', \n",
        "                       path=METRICS_LOG_PATH)\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def checkpoint(engine):\n",
        "    save_checkpoint(model, optimizer, engine.state.epoch, model_name, CHECKPOINT_PATH)\n",
        "\n",
        "@trainer.on(Events.EPOCH_COMPLETED)\n",
        "def log_validation_results(engine):\n",
        "    evaluator.run(smaller_val_dl)\n",
        "    metrics = evaluator.state.metrics\n",
        "\n",
        "    epoch = engine.state.epoch\n",
        "    #iter = evaluator.state.iteration\n",
        "    val_loss = metrics['l2_loss']\n",
        "    val_psnr = metrics['psnr']\n",
        "    val_ssim = metrics['ssim']\n",
        "    val_fid = metrics['fid']\n",
        "\n",
        "    metrics_array = np.array([val_loss, val_psnr, val_ssim, val_fid])\n",
        "\n",
        "    log_metrics_to_csv(epoch, -1, metrics_array,\n",
        "                       header='epoch,iter,loss,psnr,ssim,fid',\n",
        "                       filename='val_metrics',\n",
        "                       path=METRICS_LOG_PATH)\n",
        "\n",
        "    print(f\"Validation Results - Epoch: {epoch} Avg loss: {val_loss:.5f} PSNR: {val_psnr:.5f} SSIM: {val_ssim:.5f} FID: {val_fid:.5f}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "wDB9BF2xHRdB",
        "outputId": "31357c96-37c0-4625-cd1b-d9d68f978393"
      },
      "source": [
        "trainer.run(smaller_test_dl, max_epochs=3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch[1] Iter[1] Loss: 0.10112\n",
            "Epoch[1] Iter[2] Loss: 139910.15625\n",
            "Epoch[1] Iter[3] Loss: 0.28993\n",
            "Epoch[1] Iter[4] Loss: 0.10436\n",
            "Creating checkpoint...\n",
            "Done! Created checkpoint file: drive/MyDrive/DeepPaint/Checkpoints/dyunet_E1_10-09-2021_18-18-14.pth\n",
            "\n",
            "Validation Results - Epoch: 1 Avg loss: 0.01153 PSNR: 69.15305 SSIM: 0.99842 FID: 0.22335\n",
            "Epoch[2] Iter[5] Loss: 0.10104\n",
            "Epoch[2] Iter[6] Loss: 0.08929\n",
            "Epoch[2] Iter[7] Loss: 0.07224\n",
            "Epoch[2] Iter[8] Loss: 0.02784\n",
            "Creating checkpoint...\n",
            "Done! Created checkpoint file: drive/MyDrive/DeepPaint/Checkpoints/dyunet_E2_10-09-2021_18-18-54.pth\n",
            "\n",
            "Validation Results - Epoch: 2 Avg loss: 0.01387 PSNR: 68.38532 SSIM: 0.99812 FID: 0.23399\n",
            "Epoch[3] Iter[9] Loss: 0.02907\n",
            "Epoch[3] Iter[10] Loss: 0.03760\n",
            "Epoch[3] Iter[11] Loss: 0.02655\n",
            "Epoch[3] Iter[12] Loss: 0.02502\n",
            "Creating checkpoint...\n",
            "Done! Created checkpoint file: drive/MyDrive/DeepPaint/Checkpoints/dyunet_E3_10-09-2021_18-19-34.pth\n",
            "\n",
            "Validation Results - Epoch: 3 Avg loss: 0.00988 PSNR: 70.33888 SSIM: 0.99866 FID: 0.20478\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "State:\n",
              "\titeration: 12\n",
              "\tepoch: 3\n",
              "\tepoch_length: 4\n",
              "\tmax_epochs: 3\n",
              "\toutput: 0.025022076442837715\n",
              "\tbatch: <class 'list'>\n",
              "\tmetrics: <class 'dict'>\n",
              "\tdataloader: <class 'torch.utils.data.dataloader.DataLoader'>\n",
              "\tseed: <class 'NoneType'>\n",
              "\ttimes: <class 'dict'>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMOZvYSYLEm2"
      },
      "source": [
        "# Clear GPU cache (useful after a crash during training)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}